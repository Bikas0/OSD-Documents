{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8027583,"sourceType":"datasetVersion","datasetId":4731199},{"sourceId":8027598,"sourceType":"datasetVersion","datasetId":4731212}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.misc\nfrom zipfile import ZipFile\nfrom io import BytesIO\nimport pandas as pd\n\n# Image manipulation.\nimport PIL.Image\nfrom IPython.display import display\n\n#loading and checking the format of the ground truth csv\ntrain_truth = pd.read_csv('/kaggle/input/conv-labels/converted_file.csv')\ntrain_truth.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-04T11:00:35.197704Z","iopub.execute_input":"2024-04-04T11:00:35.198516Z","iopub.status.idle":"2024-04-04T11:00:36.345607Z","shell.execute_reply.started":"2024-04-04T11:00:35.19848Z","shell.execute_reply":"2024-04-04T11:00:36.344553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_truth.label.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-04T11:00:41.103688Z","iopub.execute_input":"2024-04-04T11:00:41.10464Z","iopub.status.idle":"2024-04-04T11:00:41.119072Z","shell.execute_reply.started":"2024-04-04T11:00:41.104606Z","shell.execute_reply":"2024-04-04T11:00:41.118139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\n\nclass CustomDataset(Dataset):\n    def __init__(self, csv_file, root_dir, transform=None):\n        self.data_frame = pd.read_csv(csv_file)\n        self.data_frame = self.data_frame.dropna(subset=['label'])  # Remove rows with missing labels\n        self.root_dir = root_dir\n        self.transform = transform\n        self.label_map = {'upside_down': 0, 'rotated_left': 1, 'rotated_right': 2, 'upright': 3}  # Define your label mapping\n\n    def __len__(self):\n        return len(self.data_frame)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.root_dir, self.data_frame.iloc[idx, 0])\n        image = Image.open(img_name)\n        label = self.label_map[self.data_frame.iloc[idx, 1]]  # Map label to integer using the dictionary\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, label\n\n\n\n# Define transformations for data augmentation\ntransform = transforms.Compose([\n    transforms.Resize((32, 32)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Assuming 3 channels\n])\n\n# Define dataset\ncustom_dataset = CustomDataset(csv_file=\"/kaggle/input/conv-labels/converted_file.csv\",\n                               root_dir=\"/kaggle/input/imageds/myDS\",\n                               transform=transform)\n\n# Split dataset into train and validation\ntrain_size = int(0.8 * len(custom_dataset))\nval_size = len(custom_dataset) - train_size\ntrain_dataset, validation_dataset = torch.utils.data.random_split(custom_dataset, [train_size, val_size])\n\n# Create data loaders\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\nvalidation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=32, shuffle=False)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-04T11:01:13.639938Z","iopub.execute_input":"2024-04-04T11:01:13.640406Z","iopub.status.idle":"2024-04-04T11:01:20.726532Z","shell.execute_reply.started":"2024-04-04T11:01:13.640372Z","shell.execute_reply":"2024-04-04T11:01:20.725126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# Define the CNN architecture\nclass CNN(nn.Module):\n    def __init__(self, num_classes=4):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 32, kernel_size=3)\n        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.conv4 = nn.Conv2d(64, 64, kernel_size=3)\n        self.fc1 = nn.Linear(64 * 5 * 5, 512)  # Correct size calculation\n        self.fc2 = nn.Linear(512, num_classes)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.dropout = nn.Dropout(0.5)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.pool(self.relu(self.conv1(x)))\n        x = self.relu(self.conv2(x))\n        x = self.dropout(x)\n        x = self.pool(self.relu(self.conv3(x)))\n        x = self.relu(self.conv4(x))\n        x = self.dropout(x)\n        #print(x.size())  # Print the size of the tensor after the fourth convolutional layer\n        x = x.view(-1, 64 * 5 * 5)  # Correct reshaping\n        x = self.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-04-04T11:01:27.978217Z","iopub.execute_input":"2024-04-04T11:01:27.978692Z","iopub.status.idle":"2024-04-04T11:01:27.993082Z","shell.execute_reply.started":"2024-04-04T11:01:27.978656Z","shell.execute_reply":"2024-04-04T11:01:27.991667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform_train = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\ntransform_val = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])","metadata":{"execution":{"iopub.status.busy":"2024-04-04T11:01:36.294327Z","iopub.execute_input":"2024-04-04T11:01:36.294758Z","iopub.status.idle":"2024-04-04T11:01:36.302639Z","shell.execute_reply.started":"2024-04-04T11:01:36.294723Z","shell.execute_reply":"2024-04-04T11:01:36.301254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load CIFAR-10 dataset\ntrain_dataset = train_dataset\nval_dataset = validation_dataset\n\n#train_dataset.dataset.transform = transform_train\n#validation_dataset.dataset.transform = transform_val\n\n# Define data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-04T11:01:43.28425Z","iopub.execute_input":"2024-04-04T11:01:43.284733Z","iopub.status.idle":"2024-04-04T11:01:43.294468Z","shell.execute_reply.started":"2024-04-04T11:01:43.284702Z","shell.execute_reply":"2024-04-04T11:01:43.292971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = CNN()\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.RMSprop(model.parameters(), lr=0.0001, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n\n# Move the model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nmodel.to(device)\n\ntrain_losses = []\nvalid_losses = []\ntrain_accuracies = []\nvalid_accuracies = []\n\n# Training the model\nnum_epochs = 10000\nbest_val_acc = 0.0  # Variable to keep track of best validation accuracy\nbest_model_path = None\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    print(f\"Epoch {epoch+1}/{num_epochs}\")\n    for i, (inputs, labels) in enumerate(train_loader):\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n\n        if (i+1) % 10 == 0:  # Print every 10 batches\n            avg_loss = running_loss / total\n            accuracy = correct / total\n            # print(f\"Batch [{i+1}/{len(train_loader)}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n\n    epoch_loss = running_loss / len(train_loader)\n    epoch_acc = correct / total\n    # print(f\"Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_acc:.4f}\")\n\n    # Validation\n    model.eval()\n    val_loss = 0.0\n    val_correct = 0\n    val_total = 0\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            val_loss += loss.item()\n            _, predicted = outputs.max(1)\n            val_total += labels.size(0)\n            val_correct += predicted.eq(labels).sum().item()\n\n    val_epoch_loss = val_loss / len(val_loader)\n    val_epoch_acc = val_correct / val_total\n    # print(f\"Val Loss: {val_epoch_loss:.4f} | Val Acc: {val_epoch_acc:.4f}\")\n    train_losses.append(epoch_loss)\n    valid_losses.append(val_epoch_loss)\n    train_accuracies.append(epoch_acc)\n    valid_accuracies.append(val_epoch_acc)\n    print(f\"Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_acc:.4f} | Val Loss: {val_epoch_loss:.4f} | Val Acc: {val_epoch_acc:.4f}\")\n\n    # Save the model if the current validation accuracy is better than the previous best\n    if val_epoch_acc > best_val_acc:\n        if best_model_path:\n            os.remove(best_model_path)  # Remove previously saved best model\n        best_val_acc = val_epoch_acc\n        best_model_path = f\"best_model_{val_epoch_acc:.4f}_valACC_{num_epochs}_epoch.pth\"\n        torch.save(model.state_dict(), best_model_path)\n\nprint(f\"Best validation accuracy: {best_val_acc}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-04T11:05:15.405022Z","iopub.execute_input":"2024-04-04T11:05:15.405404Z","iopub.status.idle":"2024-04-04T11:05:58.987162Z","shell.execute_reply.started":"2024-04-04T11:05:15.405375Z","shell.execute_reply":"2024-04-04T11:05:58.986169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nepochs = range(1, len(train_losses) + 1)\n\n# Plot losses\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.plot(epochs, train_losses, 'bo-', label='Training loss')\nplt.plot(epochs, valid_losses, 'ro-', label='Validation loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\n# Plot accuracies\nplt.subplot(1, 2, 2)\nplt.plot(epochs, train_accuracies, 'bo-', label='Training accuracy')\nplt.plot(epochs, valid_accuracies, 'ro-', label='Validation accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-04T11:06:38.582764Z","iopub.execute_input":"2024-04-04T11:06:38.583226Z","iopub.status.idle":"2024-04-04T11:06:39.338171Z","shell.execute_reply.started":"2024-04-04T11:06:38.583192Z","shell.execute_reply":"2024-04-04T11:06:39.337305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}